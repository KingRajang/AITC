{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Analysis and Visualization\n",
    "\n",
    "**Purpose:** This notebook is for evaluating the performance of the trained Q-Learning agent against a baseline (fixed-time) controller and analyzing the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries and Load Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "from src.simulation.environment import JammingMachine\n",
    "from src.agent.q_learning_agent import QLearningAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIM_CONFIG_PATH = \"../config/sim_config.json\"\n",
    "Q_TABLE_PATH = \"../trained_q_table.json\"\n",
    "LOG_FILE_PATH = \"../training_log.json\"\n",
    "NUM_EVALUATION_EPISODES = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Analyze Training Progress (The Learning Curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    log_df = pd.read_json(LOG_FILE_PATH)\n",
    "    \n",
    "    # Calculate a moving average to smooth the curve and see the trend\n",
    "    log_df['reward_moving_avg'] = log_df['total_reward'].rolling(window=100).mean()\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot the smoothed reward curve\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Total Reward (Smoothed)', color=color)\n",
    "    ax1.plot(log_df['episode'], log_df['reward_moving_avg'], color=color, label='Smoothed Reward')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Create a second y-axis to show the epsilon decay\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:red'\n",
    "    ax2.set_ylabel('Epsilon (Exploration Rate)', color=color)\n",
    "    ax2.plot(log_df['episode'], log_df['epsilon'], color=color, linestyle='--', label='Epsilon')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    plt.title('Agent Training Progress Over Episodes')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Log file not found at {LOG_FILE_PATH}. Please run the training script first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Load Environment and Trained Agent for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SIM_CONFIG_PATH) as f:\n",
    "    sim_config = json.load(f)\n",
    "env = JammingMachine(sim_config)\n",
    "trained_agent = QLearningAgent(state_space_size=8, action_space_size=6, learning_rate=0, discount_factor=0.95, exploration_rate=0)\n",
    "trained_agent.load_q_table(Q_TABLE_PATH)\n",
    "print(\"Environment and trained agent loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define Baseline Agent and Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedTimeAgent:\n",
    "    def __init__(self):\n",
    "        self.action_sequence = [1, 4] # Normal NS, Normal EW\n",
    "        self.steps_per_action = 30\n",
    "        self.current_step = 0\n",
    "        self.action_index = 0\n",
    "    def choose_action(self, state):\n",
    "        if self.current_step > 0 and self.current_step % self.steps_per_action == 0:\n",
    "            self.action_index = (self.action_index + 1) % len(self.action_sequence)\n",
    "        self.current_step += 1\n",
    "        return self.action_sequence[self.action_index]\n",
    "\n",
    "def evaluate_agent(agent, environment, num_episodes):\n",
    "    total_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        state = environment.reset()\n",
    "        # Reset the agent's internal state if it has one (like the fixed-time agent)\n",
    "        if hasattr(agent, 'current_step'): agent.current_step = 0\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            state, reward, done, info = environment.step(action)\n",
    "            episode_reward += reward\n",
    "        total_rewards.append(episode_reward)\n",
    "    return {'avg_reward': np.mean(total_rewards), 'std_reward': np.std(total_rewards)}\n",
    "\n",
    "print(\"Evaluating agents...\")\n",
    "rl_agent_results = evaluate_agent(trained_agent, env, NUM_EVALUATION_EPISODES)\n",
    "baseline_agent = FixedTimeAgent()\n",
    "baseline_results = evaluate_agent(baseline_agent, env, NUM_EVALUATION_EPISODES)\n",
    "print(\"Evaluation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Display and Plot Final Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- PERFORMANCE COMPARISON ---\")\n",
    "print(f\"Metric: Average Total Reward per Episode (Higher is better)\")\n",
    "print(f\"  - Trained RL Agent: {rl_agent_results['avg_reward']:,.2f} (+/- {rl_agent_results['std_reward']:,.2f})\")\n",
    "print(f\"  - Baseline Agent:   {baseline_results['avg_reward']:,.2f} (+/- {baseline_results['std_reward']:,.2f})\")\n",
    "improvement = (rl_agent_results['avg_reward'] - baseline_results['avg_reward']) / abs(baseline_results['avg_reward']) * 100\n",
    "print(f\"\\nImprovement by RL Agent: {improvement:.2f}%\")\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "labels = ['Trained RL Agent', 'Baseline Fixed-Time']\n",
    "means = [rl_agent_results['avg_reward'], baseline_results['avg_reward']]\n",
    "stds = [rl_agent_results['std_reward'], baseline_results['std_reward']]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "ax.bar(labels, means, yerr=stds, align='center', alpha=0.7, ecolor='black', capsize=10, color=['#4CAF50', '#F44336'])\n",
    "ax.set_ylabel('Total Reward (Cumulative Score)')\n",
    "ax.set_title('Performance Comparison: AI Agent vs. Fixed-Time Controller')\n",
    "ax.yaxis.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
